{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from typing import Literal, NamedTuple\n",
    "import os\n",
    "import numpy as np\n",
    "import rtdl_num_embeddings  # https://github.com/yandex-research/rtdl-num-embeddings\n",
    "import scipy.special\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "from tabm_reference import Model, make_parameter_groups\n",
    "import polars as pl\n",
    "import pandas as pd \n",
    "import gc\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "warnings.resetwarnings()\n",
    "import time\n",
    "import wandb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ResourceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_r2_by_periods(df, y_true_col, y_pred_col, weight_col, date_col='date_id', period_days=15):\n",
    "    df = df.sort_values(date_col)\n",
    "    date_range = df[date_col].unique()\n",
    "    date_range.sort()\n",
    "    results = []\n",
    "    for start_idx in range(0, len(date_range), period_days):\n",
    "        end_idx = start_idx + period_days\n",
    "        if end_idx <= len(date_range):\n",
    "            period_dates = date_range[start_idx:end_idx]\n",
    "            period_data = df[df[date_col].isin(period_dates)]\n",
    "            \n",
    "            if len(period_data) > 0:\n",
    "                weights = period_data[weight_col].values\n",
    "                y_true = period_data[y_true_col].values\n",
    "                y_pred = period_data[y_pred_col].values\n",
    "                \n",
    "                weights = weights / np.sum(weights)\n",
    "                \n",
    "                y_true_mean = np.sum(weights * y_true)\n",
    "                numerator = np.sum(weights * (y_true - y_pred) ** 2)\n",
    "                denominator = np.sum(weights * (y_true - y_true_mean) ** 2)\n",
    "                \n",
    "                weighted_r2 = 1 - (numerator / denominator)\n",
    "                \n",
    "                results.append({\n",
    "                    'period_start': period_dates[0],\n",
    "                    'period_end': period_dates[-1],\n",
    "                    'weighted_r2': weighted_r2,\n",
    "                    'n_samples': len(period_data)\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_daily_stats(train_df, target_col):\n",
    "    daily_avg = (\n",
    "        train_df\n",
    "        .group_by([\"symbol_id\", \"date_id\"])\n",
    "        .agg([pl.col(target_col).mean().alias(\"daily_avg\"), \n",
    "            pl.col(target_col).std().alias(\"daily_std\"),\n",
    "            pl.col(target_col).min().alias(\"daily_min\"),\n",
    "            pl.col(target_col).max().alias(\"daily_max\"),\n",
    "            pl.col(target_col).median().alias(\"daily_median\"),\n",
    "            pl.col(target_col).skew().alias(\"daily_skew\"),\n",
    "            pl.col(target_col).kurtosis().alias(\"daily_kurtosis\"),\n",
    "            pl.col(target_col).last().alias(\"last_value\"),\n",
    "            pl.col(target_col).first().alias(\"first_value\"),\n",
    "            (pl.col(target_col).max() - pl.col(target_col).min()).alias(\"daily_range\"),\n",
    "            pl.col(target_col).sum().alias(\"target_sum\"),\n",
    "            pl.col(target_col).count().alias(\"daily_count\")\n",
    "            ]\n",
    "            )\n",
    "    )\n",
    "    daily_avg = daily_avg.sort([\"symbol_id\", \"date_id\"])\n",
    "\n",
    "    columns_to_shift = [\"daily_avg\", \"daily_std\", \"daily_min\", \"daily_max\", \"daily_median\", \"daily_skew\", \"daily_kurtosis\", \"last_value\", \"first_value\",\n",
    "    \"daily_range\", \"target_sum\", \"date_id\", \"daily_count\"]\n",
    "\n",
    "    daily_avg = daily_avg.with_columns([\n",
    "        pl.col(col_name)\n",
    "        .shift(1)\n",
    "        .over(\"symbol_id\")\n",
    "        .alias(f\"lag_1_{col_name}_{target_col}\")\n",
    "        for col_name in columns_to_shift\n",
    "    ])\n",
    "\n",
    "    daily_avg = daily_avg.with_columns(\n",
    "    (pl.col(\"date_id\") - pl.col(\"lag_1_date_id_responder_6\"))\n",
    "    .alias(\"days_since_lag_1\")\n",
    "    )\n",
    "\n",
    "    s1 = [f\"lag_1_{col_name}_{target_col}\" for col_name in columns_to_shift if col_name != \"date_id\"]\n",
    "\n",
    "    selected_cols = [\"symbol_id\",\"date_id\", \"days_since_lag_1\"] + s1\n",
    "    daily_avg = daily_avg.select(selected_cols)\n",
    "\n",
    "    train_df = train_df.join(daily_avg,\n",
    "              on=[\"symbol_id\", \"date_id\"],\n",
    "              how=\"left\")\n",
    "\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def create_daily_stats2(train_df, target_col):\n",
    "    daily_avg = (\n",
    "        train_df\n",
    "        .group_by([\"symbol_id\", \"date_id\"])\n",
    "        .agg([pl.col(target_col).mean().alias(\"daily_avg\"), \n",
    "            pl.col(target_col).std().alias(\"daily_std\"),\n",
    "            pl.col(target_col).min().alias(\"daily_min\"),\n",
    "            pl.col(target_col).max().alias(\"daily_max\"),\n",
    "            pl.col(target_col).median().alias(\"daily_median\"),\n",
    "            pl.col(target_col).last().alias(\"last_value\"),\n",
    "            (pl.col(target_col).max() - pl.col(target_col).min()).alias(\"daily_range\"),\n",
    "            pl.col(target_col).sum().alias(\"target_sum\"),\n",
    "            ]\n",
    "            )\n",
    "    )\n",
    "    daily_avg = daily_avg.sort([\"symbol_id\", \"date_id\"])\n",
    "\n",
    "    columns_to_shift = [\"daily_avg\", \"daily_std\", \"daily_min\", \"daily_max\", \"daily_median\", \"last_value\",\"daily_range\", \"target_sum\"]\n",
    "\n",
    "    daily_avg = daily_avg.with_columns([\n",
    "        pl.col(col_name)\n",
    "        .shift(1)\n",
    "        .over(\"symbol_id\")\n",
    "        .alias(f\"lag_1_{col_name}_{target_col}\")\n",
    "        for col_name in columns_to_shift\n",
    "    ])\n",
    "\n",
    "\n",
    "    s1 = [f\"lag_1_{col_name}_{target_col}\" for col_name in columns_to_shift if col_name != \"date_id\"]\n",
    "\n",
    "    selected_cols = [\"symbol_id\",\"date_id\"] + s1\n",
    "    daily_avg = daily_avg.select(selected_cols)\n",
    "\n",
    "    train_df = train_df.join(daily_avg,\n",
    "              on=[\"symbol_id\", \"date_id\"],\n",
    "              how=\"left\")\n",
    "\n",
    "    train_df = train_df.sort([\"date_id\", \"time_id\", \"symbol_id\"])\n",
    "    gc.collect()\n",
    "    return train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 57s, sys: 24.1 s, total: 2min 21s\n",
      "Wall time: 9.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_list = []\n",
    "for part_in in [\"6\",\"7\",\"8\",\"9\"]:\n",
    "    part_id = part_in\n",
    "    data_dir = f\"train.parquet/partition_id={part_id}/part-0.parquet\"\n",
    "    df_list.append(pl.read_parquet(data_dir))\n",
    "    gc.collect()\n",
    "\n",
    "train_df = pl.concat(df_list)\n",
    "del df_list\n",
    "gc.collect()\n",
    "\n",
    "train_df = train_df.sort([\"symbol_id\", \"date_id\", \"time_id\"])\n",
    "responder_cols = [col for col in train_df.columns if col.startswith('responder')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 59s, sys: 1min 59s, total: 4min 59s\n",
      "Wall time: 19.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "for col_name in [\"responder_6\"]:\n",
    "    train_df = create_daily_stats(train_df, col_name)\n",
    "gc.collect()\n",
    "\n",
    "for col_name in [\"responder_0\",\"responder_1\",\"responder_2\",\"responder_3\",\"responder_4\",\"responder_5\",\"responder_7\",\"responder_8\"]:\n",
    "    train_df = create_daily_stats2(train_df, col_name)\n",
    "gc.collect()\n",
    "\n",
    "default_features = [f\"feature_{idx:02d}\" for idx in range(79)]\n",
    "train_df = train_df.with_columns(null_count = pl.sum_horizontal([pl.col(col).is_null() for col in default_features]))\n",
    "\n",
    "train_df = train_df.with_columns([\n",
    "    (2 * np.pi * pl.col(\"time_id\") / 967).sin().alias(\"sin_time_id\").cast(pl.Float32),\n",
    "    (2 * np.pi * pl.col(\"time_id\") / 967).cos().alias(\"cos_time_id\").cast(pl.Float32),\n",
    "    (2 * np.pi * pl.col(\"time_id\") / 483).sin().alias(\"sin_time_id_halfday\").cast(pl.Float32),\n",
    "    (2 * np.pi * pl.col(\"time_id\") / 483).cos().alias(\"cos_time_id_halfday\").cast(pl.Float32),\n",
    "])\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_tabm(df, feature_cols, cat_cols, add_noise = False, noise_scale = 0.01):\n",
    "    df = df.fill_nan(None)\n",
    "    df = df.fill_null(0)\n",
    "    numeric_cols = [x for x in feature_cols if not x in cat_cols]\n",
    "    cat_cardinalities = [39, 13]\n",
    "\n",
    "    y_test_np = df.filter(pl.col(\"date_id\") >= 1550).select(\"responder_6\").to_numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "    X_cont_train = df.filter(pl.col(\"date_id\") < 1550).select(numeric_cols).to_numpy().astype(np.float32)\n",
    "    X_cont_test = df.filter(pl.col(\"date_id\") >= 1550).select(numeric_cols).to_numpy().astype(np.float32)\n",
    "\n",
    "    X_cat_test = df.filter(pl.col(\"date_id\") >= 1550).select(cat_cols).to_numpy().astype(np.int64)\n",
    "    \n",
    "    test_weights = df.filter(pl.col(\"date_id\") >= 1550).select(\"weight\").to_numpy().astype(np.float32)\n",
    "\n",
    "    data_numpy = {\n",
    "        'test': {\n",
    "            'x_cont': X_cont_test,\n",
    "            'x_cat': X_cat_test,\n",
    "            'y': y_test_np.astype(np.float32),\n",
    "            'weights': test_weights,\n",
    "        },\n",
    "        }\n",
    "\n",
    "    if add_noise:\n",
    "        noise = (\n",
    "            np.random.default_rng(0)\n",
    "            .normal(0.0, noise_scale, X_cont_train.shape)\n",
    "            .astype(X_cont_train.dtype)\n",
    "        )\n",
    "        preprocessing = sklearn.preprocessing.StandardScaler().fit(X_cont_train + noise)\n",
    "    else:\n",
    "        preprocessing = sklearn.preprocessing.StandardScaler().fit(X_cont_train)\n",
    "    \n",
    "    for part in data_numpy:\n",
    "        data_numpy[part]['x_cont'] = preprocessing.transform(data_numpy[part]['x_cont']).astype(np.float32)\n",
    "    \n",
    "    return data_numpy, cat_cardinalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabM Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TABM_CONFIG = joblib.load(\"tabm_model_folder/tabm_v14_model_results/tabm_v14_model_config.pkl\")\n",
    "\n",
    "tabm_feature_cols = TABM_CONFIG[\"feature_cols\"]\n",
    "cat_cols =  TABM_CONFIG[\"cat_features\"]\n",
    "len(tabm_feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numpy, cat_cardinalities = prepare_data_for_tabm(train_df, tabm_feature_cols, cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABM_CONFIG = joblib.load(\"tabm_model_folder/tabm_v14_model_results/tabm_v14_model_config.pkl\")\n",
    "\n",
    "task_type = 'regression'\n",
    "n_classes = None\n",
    "class RegressionLabelStats(NamedTuple):\n",
    "    mean: float\n",
    "    std: float\n",
    "\n",
    "regression_label_stats = RegressionLabelStats(\n",
    "    mean=-0.0014552467036992311, std=0.8571650385856628\n",
    ")\n",
    "\n",
    "amp_dtype = (\n",
    "    torch.bfloat16\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else torch.float16\n",
    "    if torch.cuda.is_available()\n",
    "    else None\n",
    ")\n",
    "amp_enabled = True and amp_dtype is not None\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bins = None\n",
    "tab_model = Model(\n",
    "    n_num_features = TABM_CONFIG[\"n_cont_features\"],\n",
    "    cat_cardinalities = TABM_CONFIG[\"cat_cardinalities\"],\n",
    "    n_classes=n_classes,\n",
    "    backbone={\n",
    "        'type': 'MLP',\n",
    "        'n_blocks': 3 if bins is None else 2,\n",
    "        'd_block': TABM_CONFIG[\"d_block\"],\n",
    "        'dropout': TABM_CONFIG[\"drop_rate\"],\n",
    "    },\n",
    "    bins=bins,\n",
    "    num_embeddings=(\n",
    "        None\n",
    "        if bins is None\n",
    "        else {\n",
    "            'type': 'PiecewiseLinearEmbeddings',\n",
    "            'd_embedding': 16,\n",
    "            'activation': False,\n",
    "            'version': 'B',\n",
    "        }\n",
    "    ),\n",
    "    arch_type=TABM_CONFIG[\"model_arch\"],\n",
    "    k=TABM_CONFIG[\"model_k\"],\n",
    ")\n",
    "\n",
    "@torch.autocast(device.type, enabled=amp_enabled, dtype=amp_dtype)  # type: ignore[code]\n",
    "def apply_model(model, data, part: str, idx: Tensor) -> Tensor:\n",
    "    return (\n",
    "        model(\n",
    "            data[part]['x_cont'][idx],\n",
    "            data[part]['x_cat'][idx] if 'x_cat' in data[part] else None,\n",
    "        )\n",
    "        .squeeze(-1)\n",
    "        .float()\n",
    "    )\n",
    "\n",
    "def inference_step(model, batch_size, device, data_numpy):\n",
    "    global regression_label_stats\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    data = {part: {k: torch.as_tensor(v, device=device) for k, v in data_numpy[part].items()}\n",
    "        for part in data_numpy}\n",
    "    y_preds_list = []\n",
    "    total_samples = len(data_numpy[\"test\"][\"x_cont\"])\n",
    "    step_count = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(\n",
    "            torch.arange(total_samples, device=device).split(batch_size),\n",
    "            desc=\"Testing\",\n",
    "            total=math.ceil(total_samples / batch_size),\n",
    "            leave=True,\n",
    "        ) as pbar:\n",
    "\n",
    "            for batch_idx in pbar:  \n",
    "                step_count += 1\n",
    "                y_pred = apply_model(tab_model, data, 'test', batch_idx)\n",
    "                y_preds_cpu = y_pred.detach().cpu().numpy()\n",
    "                y_preds_cpu = y_preds_cpu * regression_label_stats.std + regression_label_stats.mean\n",
    "                y_preds_cpu = y_preds_cpu.mean(axis = 1)\n",
    "                y_preds_cpu = np.clip(y_preds_cpu, -5, 5)\n",
    "\n",
    "                y_preds_list.append(y_preds_cpu)\n",
    "\n",
    "\n",
    "    y_preds_list = np.concatenate(y_preds_list)\n",
    "\n",
    "    return y_preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_120211/1261860405.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tab_model.load_state_dict(torch.load(\"tabm_experiment_models/tabm_73564_best.pt\", map_location = device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (cat_module): OneHotEncoding0d()\n",
       "  (backbone): MLP(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=170, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1-2): 2 x Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (minimal_ensemble_adapter): ScaleEnsemble()\n",
       "  (output): NLinear()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_model.load_state_dict(torch.load(\"tabm_experiment_models/tabm_73564_best.pt\", map_location = device))\n",
    "tab_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 684/684 [00:01<00:00, 390.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.009126475088499397"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_tabm = inference_step(model=tab_model, batch_size=8096, data_numpy=data_numpy, device=device)\n",
    "\n",
    "r2_score(data_numpy[\"test\"][\"y\"], y_preds_tabm, sample_weight = data_numpy[\"test\"][\"weights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cb_81156.cbm downloaded\n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "run = api.run(\"turkenm/js_catboost/my5jxd8c\")\n",
    "\n",
    "config = run.config\n",
    "\n",
    "summary = run.summary\n",
    "\n",
    "files = run.files()\n",
    "for file in files:\n",
    "    if file.name.endswith(\".cbm\"):\n",
    "        model_name = file.name\n",
    "        file.download(root = \"cb_train_models\",replace = False, exist_ok = True)\n",
    "        print(f\"{file.name} downloaded\")\n",
    "\n",
    "cb_model = CatBoostRegressor()\n",
    "cb_model.load_model(f\"cb_train_models/{model_name}\")\n",
    "cb_feature_cols = config[\"feature_cols\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = train_df.filter(pl.col(\"date_id\") >= 1550).select(cb_feature_cols).to_pandas()\n",
    "y_test = train_df.filter(pl.col(\"date_id\") >= 1550).select(\"responder_6\").to_pandas()\n",
    "test_weights = train_df.filter(pl.col(\"date_id\") >= 1550).select(\"weight\").to_pandas()\n",
    "test_data = Pool(X_test, y_test, weight = test_weights, cat_features = [\"symbol_id\", \"feature_09\", \"feature_11\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008764811167079056"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_cb = cb_model.predict(test_data)\n",
    "r2_score(y_test, y_preds_cb, sample_weight = test_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_version = \"v8\"\n",
    "lgb_model = lgb.Booster(model_file=f\"lgb_model_results/lgb_{lgb_version}_train.txt\")\n",
    "lgb_model_cols = joblib.load(f\"model_cols/lgb_{lgb_version}_model_cols.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mt/miniconda3/envs/tabm/lib/python3.10/site-packages/lightgbm/basic.py:701: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  target_dtype = np.find_common_type(df_dtypes, [])\n"
     ]
    }
   ],
   "source": [
    "X_test2 = train_df.filter(pl.col(\"date_id\") >= 1550).select(lgb_model_cols).to_pandas()\n",
    "X_test2[\"symbol_id\"] = X_test2[\"symbol_id\"].astype(\"category\")\n",
    "X_test2[\"feature_09\"] = X_test2[\"feature_09\"].astype(\"category\")\n",
    "X_test2[\"feature_10\"] = X_test2[\"feature_10\"].astype(\"category\")\n",
    "X_test2[\"feature_11\"] = X_test2[\"feature_11\"].astype(\"category\")\n",
    "y_preds_lgb = lgb_model.predict(X_test2[lgb_model_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = train_df.filter(pl.col(\"date_id\") >= 1550).select([\"date_id\",\"time_id\", \"symbol_id\",\"weight\",\"responder_6\"]).to_pandas()\n",
    "tmp_df[\"cb_preds\"] = y_preds_cb\n",
    "tmp_df[\"tabm_preds\"] = y_preds_tabm\n",
    "tmp_df[\"lgb_preds\"] = y_preds_lgb\n",
    "tmp_df[\"tabm_cb\"] = y_preds_cb * 0.5 + y_preds_tabm * 0.5\n",
    "tmp_df[\"tabm_cb_lgb\"] = (y_preds_tabm + y_preds_cb + y_preds_lgb) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_122_days = tmp_df.query(\"date_id > 1576\")\n",
    "last_40_days = tmp_df.query(\"date_id > 1658\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00903990588059611"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(last_122_days.responder_6, last_122_days.cb_preds, sample_weight = last_122_days.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010110281111514907"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(last_122_days.responder_6, last_122_days.tabm_preds, sample_weight = last_122_days.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010635510678499771"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(last_122_days.responder_6, last_122_days.tabm_cb, sample_weight = last_122_days.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0076311550928545024"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(last_122_days.responder_6, last_122_days.lgb_preds, sample_weight = last_122_days.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010079655363787854"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(last_122_days.responder_6, last_122_days.tabm_cb_lgb, sample_weight = last_122_days.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007758484005299882"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(last_40_days.responder_6, last_40_days.cb_preds, sample_weight = last_40_days.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008220072440771498"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(last_40_days.responder_6, last_40_days.tabm_preds, sample_weight = last_40_days.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007381033884956034"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(last_40_days.responder_6, last_40_days.lgb_preds, sample_weight = last_40_days.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008761453858847412"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(last_40_days.responder_6, last_40_days.tabm_cb_lgb, sample_weight = last_40_days.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008871022450225863"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(last_40_days.responder_6, last_40_days.tabm_cb, sample_weight = last_40_days.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008220072440771498"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(last_40_days.responder_6, last_40_days.tabm_preds, sample_weight = last_40_days.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period_start</th>\n",
       "      <th>period_end</th>\n",
       "      <th>weighted_r2</th>\n",
       "      <th>n_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1550</td>\n",
       "      <td>1579</td>\n",
       "      <td>0.004870</td>\n",
       "      <td>1116104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1580</td>\n",
       "      <td>1609</td>\n",
       "      <td>0.015151</td>\n",
       "      <td>1109328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1610</td>\n",
       "      <td>1639</td>\n",
       "      <td>0.006937</td>\n",
       "      <td>1111264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1640</td>\n",
       "      <td>1669</td>\n",
       "      <td>0.007288</td>\n",
       "      <td>1116104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   period_start  period_end  weighted_r2  n_samples\n",
       "0          1550        1579     0.004870    1116104\n",
       "1          1580        1609     0.015151    1109328\n",
       "2          1610        1639     0.006937    1111264\n",
       "3          1640        1669     0.007288    1116104"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_weighted_r2_by_periods(tmp_df, \"responder_6\", \"tabm_preds\", \"weight\", period_days = 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
